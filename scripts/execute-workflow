#!/usr/bin/env python

"""Execute a Cloudify Workflow on a Mist.io Stack.

This executable takes care of fetching the Stack's Template, unpacking it, if
it is a tarball/zip file, and actually executing the instructed orchestration
workflow using the MistClient and Cloudify CLI.

"""

import os
import sys
import json
import copy
import shlex
import random
import shutil
import urllib
import tarfile
import zipfile
import logging
import requests
import argparse
import subprocess

from mistclient import MistClient


requests.packages.urllib3.disable_warnings(
    requests.packages.urllib3.exceptions.InsecureRequestWarning
)

log = logging.getLogger(__name__)


ABSOLUTE_WORKING_DIR_PATH = '/tmp/templates/'
RELATIVE_LOCAL_STORE_PATH = 'local-storage/local/node-instances/'


def main():
    """
    The main function handling the execution of a particular workflow.
    The function retrieves the stack with all the necessary information, 
    including template source and inputs.
    It initializes cloudify and then runs the relevant workflow."""

    args = parse_args()
    prepare_logging(args.verbose)

    exit_code = None
    cmdret = []
    node_instances = []
    outputs = {}

    try:
        # Instantiate a new MistClient.
        client = MistClient(api_token=args.api_token, mist_uri=args.uri)

        # Fetch the Stack specified and its corresponding Template.
        stack = client.show_stack(args.stack_id)
        template = client.show_template(stack['template'])

        inputs = stack["inputs"]

        if 'mist_uri' in inputs['install']:
            inputs['install']['mist_uri'] = args.uri
        else:
            raise Exception('mist_uri input is missing.'
                            'This is a required parameter.')
        if 'mist_token' in inputs['install']:
            inputs['install']['mist_token'] = args.api_token
        else:
            raise Exception('mist_token input is missing.'
                            'This is a required parameter.')
        
        # Storing the job ID corresponding to the particular Stack in order to
        # create nested logs during the execution of Cloudify workflows. The
        # logs generated by each workflow will use this job ID as their origin
        # TODO: Pass as ENV variables.
        with open('/tmp/cloudify-mist-plugin-job', 'w') as jf:
            jf.write(str(stack['job_id']))
        # Store the Stack's name
        with open('/tmp/cloudify-mist-plugin-stack', 'w') as sf:
            sf.write(str(stack['name']))

        if not os.path.exists('/tmp/templates'):
            os.makedirs('/tmp/templates')
        if template['location_type'] == 'inline':
            path = os.path.abspath('/tmp/templates/template.yaml')
            with open(path, 'w+') as fobj:
                fobj.write(template['template'])
        elif template['location_type'] == 'github':
            repo = template['template'].replace('https://github.com/', '')
            repo_split = repo.split('tree/')
            repo = repo_split[0].rstrip('/')
            branch = repo_split[1].rstrip('/') if len(repo_split) > 1 else 'master'

            api = 'https://api.github.com/repos'
            resp = requests.get('%s/%s/commits' % (api, repo))
            if not resp.ok:
                raise Exception('Failed to retrieve template from Github.')
            try:
                resp = resp.json()
            except ValueError as exc:
                log.error('Failed to decode due to %s error. Congrats!' % str(exc))
                raise 

            resp = requests.get('%s/%s/tarball/%s' % (api, repo, branch),
                                allow_redirects=False)
            if not (resp.ok and resp.is_redirect and 'location' in resp.headers):
                raise Exception('Failed to fetch Github repo tarball.')

            path = download(resp.headers['location'])
            try:
                unpack(path, '/tmp/templates/')
            except Exception as exc:
                log.error(exc)
            else:
                path = find_path(template['entrypoint'], '/tmp/templates')
        elif template['location_type'] == 'url':
            raise NotImplementedError()
        else:
            raise Exception('Unsupported template location. It must be one '
                            'of url|github|inline')


        folder = os.path.dirname(path)
        os.chdir(folder)
        local_instances = os.path.abspath(
            os.path.join(folder, RELATIVE_LOCAL_STORE_PATH))

        # Prepare inputs.
        with open('inputs.json', 'wb') as inputs_json:
            inputs_json.write(json.dumps(inputs.get('install', {})))

        # Initialize a new cloudify environment.
        cmd = 'cfy local init -p %s -i inputs.json' % path
        cmdret.append(run(cmd))

        # Override local-storage with existing node instances. The node
        # instances are loaded from db and are re-stored locally during
        # the execution of any workflow on an existing Stack so that we
        # can always access the entire set of instances, even though we
        # are re-initializing the cloudify environment each time we are
        # running a new workflow.
        if stack['node_instances']:
            shutil.rmtree(RELATIVE_LOCAL_STORE_PATH)
            os.mkdir(RELATIVE_LOCAL_STORE_PATH)
            for instance in stack['node_instances']:
                with open(os.path.join(local_instances, instance['id']), 'w') as ifile:
                    ifile.write(json.dumps(instance))

        # Execute the cloudify workflow and store its output.
        cmd = 'cfy local execute -w %s' % args.workflow
        # Prepare non-install workflow specific inputs.
        if args.workflow != 'install' and inputs.get(args.workflow):
            with open('workflow_inputs.json', 'wb') as workflow_inputs:
                workflow_inputs.write(json.dumps(inputs.get(args.workflow, {})))
            cmd = cmd + " -p workflow_inputs.json"
        cmdret.append(run(cmd, fail_on_error=False))

        # TODO: Could we use `cfy local instances`?
        # Load the new set of instances in order to be stored in the db for
        # future reference.
        for instance in os.listdir(local_instances):
            node_path = os.path.join(folder, RELATIVE_LOCAL_STORE_PATH, instance)
            with open(os.path.abspath(node_path), 'r') as instance:
                node_instances.append(json.load(instance))

        # Get the template's output, if available.
        if not exit_code:
            outputs = run('cfy local outputs', fail_on_error=False)
            cmdret.append(outputs)
            outputs = json.loads(outputs[1])
    except Exception as exc:
        exit_code = 1
        error = repr(exc)
        log.critical(error)
    else:
        exit_code = cmdret[-1][0]
        error = None

    log.info('Wrapper script execution finished with exit code %s' % exit_code)
    client.end_job(stack['job_id'],
                   exit_code=exit_code,
                   cmdout=cmdret,
                   error=error or exit_code or False,
                   node_instances=node_instances,
                   outputs=outputs)

    return exit_code


# TODO
def run(cmd, shell=False, fail_on_error=True):
    """Run a command using the shell."""

    def stream(proc):
        for line in iter(proc.stdout.readline, ''):
            yield line

    log.debug("Running command '%s'", cmd)
    cmd = shlex.split(cmd)
    p = subprocess.Popen(cmd, shell=shell, universal_newlines=True,
                         stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    out = ''
    for next_line in stream(p):
        out += next_line
        sys.stdout.write(next_line)
        sys.stdout.flush()

    _, err = p.communicate()  # `out` is not returned by communicate()
    if p.returncode:
        err = "Command exited with return code %d: %s" % (p.returncode, err)
        log.error(err)
        if fail_on_error:
            raise Exception(err)

    return p.returncode, out, err


def download(url, path=None):
    """Download a file over HTTP."""
    name, headers = urllib.urlretrieve(url, path)
    log.debug('Downloaded %s to %s', url, name)
    return name


def unpack(filename, dirname='.'):
    """Unpack a tarball or zip archive."""
    dirname = os.path.abspath(dirname)
    if not os.path.isdir(dirname):
        raise TypeError('%s is not a directory' % dirname)
    if tarfile.is_tarfile(filename):
        log.debug('Unpacking %s tarball in %s directory', filename, dirname)
        tfile = tarfile.open(filename)
        tfile.extractall(dirname)
    elif zipfile.is_zipfile(filename):
        log.debug('Unpacking %s zip in %s directory', filename, dirname)
        zfile = zipfile.ZipFile(filename)
        zfile.extractall(dirname)
    else:
        raise TypeError('File %s is not a valid tar or zip archive' % filename)


def find_path(filename, dirname='.'):
    """Find the absolute path of a file."""
    dirname = os.path.abspath(dirname)
    if os.path.isdir(dirname):
        for dirpath, dirnames, filenames in os.walk(dirname):
            if filename in filenames:
                log.debug('Found file %s under %s', filenames, dirpath)
                return os.path.abspath(os.path.join(dirpath, filename))
        raise Exception('Failed to locate %s under %s', filename, dirname)
    raise Exception('%s is not a directory' % dirname)


def prepare_logging(verbose=False):
    """Set logging configuration."""
    logfmt = "[%(asctime)-15s][%(levelname)s] - %(message)s"
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter(logfmt))
    log.addHandler(handler)
    log.setLevel(logging.DEBUG if verbose else logging.INFO)


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Execute a workflow on a Mist.io Stack",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "stack_id",
        help="The ID of the Mist.io Stack to execute the workflow on"
    )
    parser.add_argument(
        "-w", "--workflow", type=str, default="install",
        help="The workflow to execute. Defaults to 'install'"
    )
    parser.add_argument(
        "-u", "--uri", type=str, default="https://mist.io",
        help="The URI of the Mist.io service to connect to. Defaults to the "
             "Mist.io SaaS at https://mist.io"
    )
    parser.add_argument(
        "-t", "--api-token", type=str, required=True,
        help="The API token to be used for authentication. Since workflows "
             "are executed in dedicated docker containers, an API token must "
             "be provided in order to authenticate to the Mist.io API, while "
             "performing API calls. These tokens are normal API tokens, bound "
             "to the requesting User, which results in as if the User himself "
             "performed the actions defined by the workflow"
    )
    parser.add_argument(
        "-v", "--verbose", action="store_true",
        help="Show debug logs. Defaults to INFO level logging"
    )

    return parser.parse_args()


if __name__ == "__main__":
    sys.exit(main())
